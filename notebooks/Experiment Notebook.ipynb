{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b9ef2b",
   "metadata": {},
   "source": [
    "# Keyword Analysis with KeyBERT and Taipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be6d89b",
   "metadata": {},
   "source": [
    "## 01 - Extraction of arXiv Abstracts with API\n",
    "- https://github.com/lukasschwab/arxiv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53bdba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import ast\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65228909",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(\n",
    "            query = 'artificial intelligence',\n",
    "            max_results = 20,\n",
    "            sort_by = arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order = arxiv.SortOrder.Descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b3f36ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arxiv.arxiv.Search"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3875f5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/abs/2303.13519v1\n",
      "2023-03-23 17:59:54+00:00\n",
      "Learning and Verification of Task Structure in Instructional Videos\n",
      "Given the enormous number of instructional videos available online, learning\n",
      "a diverse array of multi-step task models from videos is an appealing goal. We\n",
      "introduce a new pre-trained video model, VideoTaskformer, focused on\n",
      "representing the semantics and structure of instructional videos. We pre-train\n",
      "VideoTaskformer using a simple and effective objective: predicting weakly\n",
      "supervised textual labels for steps that are randomly masked out from an\n",
      "instructional video (masked step modeling). Compared to prior work which learns\n",
      "step representations locally, our approach involves learning them globally,\n",
      "leveraging video of the entire surrounding task as context. From these learned\n",
      "representations, we can verify if an unseen video correctly executes a given\n",
      "task, as well as forecast which steps are likely to be taken after a given\n",
      "step. We introduce two new benchmarks for detecting mistakes in instructional\n",
      "videos, to verify if there is an anomalous step and if steps are executed in\n",
      "the right order. We also introduce a long-term forecasting benchmark, where the\n",
      "goal is to predict long-range future steps from a given step. Our method\n",
      "outperforms previous baselines on these tasks, and we believe the tasks will be\n",
      "a valuable way for the community to measure the quality of step\n",
      "representations. Additionally, we evaluate VideoTaskformer on 3 existing\n",
      "benchmarks -- procedural activity recognition, step classification, and step\n",
      "forecasting -- and demonstrate on each that our method outperforms existing\n",
      "baselines and achieves new state-of-the-art performance.\n",
      "http://arxiv.org/abs/2303.13518v1\n",
      "2023-03-23 17:59:53+00:00\n",
      "Three ways to improve feature alignment for open vocabulary detection\n",
      "The core problem in zero-shot open vocabulary detection is how to align\n",
      "visual and text features, so that the detector performs well on unseen classes.\n",
      "Previous approaches train the feature pyramid and detection head from scratch,\n",
      "which breaks the vision-text feature alignment established during pretraining,\n",
      "and struggles to prevent the language model from forgetting unseen classes.\n",
      "  We propose three methods to alleviate these issues. Firstly, a simple scheme\n",
      "is used to augment the text embeddings which prevents overfitting to a small\n",
      "number of classes seen during training, while simultaneously saving memory and\n",
      "computation. Secondly, the feature pyramid network and the detection head are\n",
      "modified to include trainable gated shortcuts, which encourages vision-text\n",
      "feature alignment and guarantees it at the start of detection training.\n",
      "Finally, a self-training approach is used to leverage a larger corpus of\n",
      "image-text pairs thus improving detection performance on classes with no human\n",
      "annotated bounding boxes.\n",
      "  Our three methods are evaluated on the zero-shot version of the LVIS\n",
      "benchmark, each of them showing clear and significant benefits. Our final\n",
      "network achieves the new stateof-the-art on the mAP-all metric and demonstrates\n",
      "competitive performance for mAP-rare, as well as superior transfer to COCO and\n",
      "Objects365.\n",
      "http://arxiv.org/abs/2303.13512v1\n",
      "2023-03-23 17:59:17+00:00\n",
      "Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the MineRL BASALT 2022 Competition\n",
      "To facilitate research in the direction of fine-tuning foundation models from\n",
      "human feedback, we held the MineRL BASALT Competition on Fine-Tuning from Human\n",
      "Feedback at NeurIPS 2022. The BASALT challenge asks teams to compete to develop\n",
      "algorithms to solve tasks with hard-to-specify reward functions in Minecraft.\n",
      "Through this competition, we aimed to promote the development of algorithms\n",
      "that use human feedback as channels to learn the desired behavior. We describe\n",
      "the competition and provide an overview of the top solutions. We conclude by\n",
      "discussing the impact of the competition and future directions for improvement.\n",
      "http://arxiv.org/abs/2303.13511v1\n",
      "2023-03-23 17:59:10+00:00\n",
      "Neural Preset for Color Style Transfer\n",
      "In this paper, we present a Neural Preset technique to address the\n",
      "limitations of existing color style transfer methods, including visual\n",
      "artifacts, vast memory requirement, and slow style switching speed. Our method\n",
      "is based on two core designs. First, we propose Deterministic Neural Color\n",
      "Mapping (DNCM) to consistently operate on each pixel via an image-adaptive\n",
      "color mapping matrix, avoiding artifacts and supporting high-resolution inputs\n",
      "with a small memory footprint. Second, we develop a two-stage pipeline by\n",
      "dividing the task into color normalization and stylization, which allows\n",
      "efficient style switching by extracting color styles as presets and reusing\n",
      "them on normalized input images. Due to the unavailability of pairwise\n",
      "datasets, we describe how to train Neural Preset via a self-supervised\n",
      "strategy. Various advantages of Neural Preset over existing methods are\n",
      "demonstrated through comprehensive evaluations. Besides, we show that our\n",
      "trained model can naturally support multiple applications without fine-tuning,\n",
      "including low-light image enhancement, underwater image correction, image\n",
      "dehazing, and image harmonization.\n",
      "http://arxiv.org/abs/2303.13508v1\n",
      "2023-03-23 17:59:00+00:00\n",
      "DreamBooth3D: Subject-Driven Text-to-3D Generation\n",
      "We present DreamBooth3D, an approach to personalize text-to-3D generative\n",
      "models from as few as 3-6 casually captured images of a subject. Our approach\n",
      "combines recent advances in personalizing text-to-image models (DreamBooth)\n",
      "with text-to-3D generation (DreamFusion). We find that naively combining these\n",
      "methods fails to yield satisfactory subject-specific 3D assets due to\n",
      "personalized text-to-image models overfitting to the input viewpoints of the\n",
      "subject. We overcome this through a 3-stage optimization strategy where we\n",
      "jointly leverage the 3D consistency of neural radiance fields together with the\n",
      "personalization capability of text-to-image models. Our method can produce\n",
      "high-quality, subject-specific 3D assets with text-driven modifications such as\n",
      "novel poses, colors and attributes that are not seen in any of the input images\n",
      "of the subject.\n",
      "http://arxiv.org/abs/2303.13497v1\n",
      "2023-03-23 17:56:20+00:00\n",
      "TriPlaneNet: An Encoder for EG3D Inversion\n",
      "Recent progress in NeRF-based GANs has introduced a number of approaches for\n",
      "high-resolution and high-fidelity generative modeling of human heads with a\n",
      "possibility for novel view rendering. At the same time, one must solve an\n",
      "inverse problem to be able to re-render or modify an existing image or video.\n",
      "Despite the success of universal optimization-based methods for 2D GAN\n",
      "inversion, those, applied to 3D GANs, may fail to produce 3D-consistent\n",
      "renderings. Fast encoder-based techniques, such as those developed for\n",
      "StyleGAN, may also be less appealing due to the lack of identity preservation.\n",
      "In our work, we introduce a real-time method that bridges the gap between the\n",
      "two approaches by directly utilizing the tri-plane representation introduced\n",
      "for EG3D generative model. In particular, we build upon a feed-forward\n",
      "convolutional encoder for the latent code and extend it with a\n",
      "fully-convolutional predictor of tri-plane numerical offsets. As shown in our\n",
      "work, the renderings are similar in quality to optimization-based techniques\n",
      "and significantly outperform the baselines for novel view. As we empirically\n",
      "prove, this is a consequence of directly operating in the tri-plane space, not\n",
      "in the GAN parameter space, while making use of an encoder-based trainable\n",
      "approach.\n",
      "http://arxiv.org/abs/2303.13496v1\n",
      "2023-03-23 17:56:12+00:00\n",
      "The effectiveness of MAE pre-pretraining for billion-scale pretraining\n",
      "This paper revisits the standard pretrain-then-finetune paradigm used in\n",
      "computer vision for visual recognition tasks. Typically, state-of-the-art\n",
      "foundation models are pretrained using large scale (weakly) supervised datasets\n",
      "with billions of images. We introduce an additional pre-pretraining stage that\n",
      "is simple and uses the self-supervised MAE technique to initialize the model.\n",
      "While MAE has only been shown to scale with the size of models, we find that it\n",
      "scales with the size of the training dataset as well. Thus, our MAE-based\n",
      "pre-pretraining scales with both model and data size making it applicable for\n",
      "training foundation models. Pre-pretraining consistently improves both the\n",
      "model convergence and the downstream transfer performance across a range of\n",
      "model scales (millions to billions of parameters), and dataset sizes (millions\n",
      "to billions of images). We measure the effectiveness of pre-pretraining on 10\n",
      "different visual recognition tasks spanning image classification, video\n",
      "recognition, object detection, low-shot classification and zero-shot\n",
      "recognition. Our largest model achieves new state-of-the-art results on\n",
      "iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on\n",
      "Food-101 (96.0%). Our study reveals that model initialization plays a\n",
      "significant role, even for web-scale pretraining with billions of images.\n",
      "http://arxiv.org/abs/2303.13494v1\n",
      "2023-03-23 17:55:32+00:00\n",
      "Attention! Dynamic Epistemic Logic Models of (In)attentive Agents\n",
      "Attention is the crucial cognitive ability that limits and selects what\n",
      "information we observe. Previous work by Bolander et al. (2016) proposes a\n",
      "model of attention based on dynamic epistemic logic (DEL) where agents are\n",
      "either fully attentive or not attentive at all. While introducing the realistic\n",
      "feature that inattentive agents believe nothing happens, the model does not\n",
      "represent the most essential aspect of attention: its selectivity. Here, we\n",
      "propose a generalization that allows for paying attention to subsets of atomic\n",
      "formulas. We introduce the corresponding logic for propositional attention, and\n",
      "show its axiomatization to be sound and complete. We then extend the framework\n",
      "to account for inattentive agents that, instead of assuming nothing happens,\n",
      "may default to a specific truth-value of what they failed to attend to (a sort\n",
      "of prior concerning the unattended atoms). This feature allows for a more\n",
      "cognitively plausible representation of the inattentional blindness phenomenon,\n",
      "where agents end up with false beliefs due to their failure to attend to\n",
      "conspicuous but unexpected events. Both versions of the model define\n",
      "attention-based learning through appropriate DEL event models based on a few\n",
      "and clear edge principles. While the size of such event models grow\n",
      "exponentially both with the number of agents and the number of atoms, we\n",
      "introduce a new logical language for describing event models syntactically and\n",
      "show that using this language our event models can be represented linearly in\n",
      "the number of agents and atoms. Furthermore, representing our event models\n",
      "using this language is achieved by a straightforward formalisation of the\n",
      "aforementioned edge principles.\n",
      "http://arxiv.org/abs/2303.13489v1\n",
      "2023-03-23 17:53:44+00:00\n",
      "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey\n",
      "Although reinforcement learning has seen tremendous success recently, this\n",
      "kind of trial-and-error learning can be impractical or inefficient in complex\n",
      "environments. The use of demonstrations, on the other hand, enables agents to\n",
      "benefit from expert knowledge rather than having to discover the best action to\n",
      "take through exploration. In this survey, we discuss the advantages of using\n",
      "demonstrations in sequential decision making, various ways to apply\n",
      "demonstrations in learning-based decision making paradigms (for example,\n",
      "reinforcement learning and planning in the learned models), and how to collect\n",
      "the demonstrations in various scenarios. Additionally, we exemplify a practical\n",
      "pipeline for generating and utilizing demonstrations in the recently proposed\n",
      "ManiSkill robot learning benchmark.\n",
      "http://arxiv.org/abs/2303.13483v1\n",
      "2023-03-23 17:50:40+00:00\n",
      "NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations\n",
      "Grounding object properties and relations in 3D scenes is a prerequisite for\n",
      "a wide range of artificial intelligence tasks, such as visually grounded\n",
      "dialogues and embodied manipulation. However, the variability of the 3D domain\n",
      "induces two fundamental challenges: 1) the expense of labeling and 2) the\n",
      "complexity of 3D grounded language. Hence, essential desiderata for models are\n",
      "to be data-efficient, generalize to different data distributions and tasks with\n",
      "unseen semantic forms, as well as ground complex language semantics (e.g.,\n",
      "view-point anchoring and multi-object reference). To address these challenges,\n",
      "we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates\n",
      "language into programs with hierarchical structures by leveraging large\n",
      "language-to-code models. Different functional modules in the programs are\n",
      "implemented as neural networks. Notably, NS3D extends prior neuro-symbolic\n",
      "visual reasoning methods by introducing functional modules that effectively\n",
      "reason about high-arity relations (i.e., relations among more than two\n",
      "objects), key in disambiguating objects in complex 3D scenes. Modular and\n",
      "compositional architecture enables NS3D to achieve state-of-the-art results on\n",
      "the ReferIt3D view-dependence task, a 3D referring expression comprehension\n",
      "benchmark. Importantly, NS3D shows significantly improved performance on\n",
      "settings of data-efficiency and generalization, and demonstrate zero-shot\n",
      "transfer to an unseen 3D question-answering task.\n",
      "http://arxiv.org/abs/2303.13482v1\n",
      "2023-03-23 17:50:09+00:00\n",
      "TactoFind: A Tactile Only System for Object Retrieval\n",
      "We study the problem of object retrieval in scenarios where visual sensing is\n",
      "absent, object shapes are unknown beforehand and objects can move freely, like\n",
      "grabbing objects out of a drawer. Successful solutions require localizing free\n",
      "objects, identifying specific object instances, and then grasping the\n",
      "identified objects, only using touch feedback. Unlike vision, where cameras can\n",
      "observe the entire scene, touch sensors are local and only observe parts of the\n",
      "scene that are in contact with the manipulator. Moreover, information gathering\n",
      "via touch sensors necessitates applying forces on the touched surface which may\n",
      "disturb the scene itself. Reasoning with touch, therefore, requires careful\n",
      "exploration and integration of information over time -- a challenge we tackle.\n",
      "We present a system capable of using sparse tactile feedback from fingertip\n",
      "touch sensors on a dexterous hand to localize, identify and grasp novel objects\n",
      "without any visual feedback. Videos are available at\n",
      "https://taochenshh.github.io/projects/tactofind.\n",
      "http://arxiv.org/abs/2303.13472v1\n",
      "2023-03-23 17:43:17+00:00\n",
      "Plotting Behind the Scenes: Towards Learnable Game Engines\n",
      "Game engines are powerful tools in computer graphics. Their power comes at\n",
      "the immense cost of their development. In this work, we present a framework to\n",
      "train game-engine-like neural models, solely from monocular annotated videos.\n",
      "The result-a Learnable Game Engine (LGE)-maintains states of the scene, objects\n",
      "and agents in it, and enables rendering the environment from a controllable\n",
      "viewpoint. Similarly to a game engine, it models the logic of the game and the\n",
      "underlying rules of physics, to make it possible for a user to play the game by\n",
      "specifying both high- and low-level action sequences. Most captivatingly, our\n",
      "LGE unlocks the director's mode, where the game is played by plotting behind\n",
      "the scenes, specifying high-level actions and goals for the agents in the form\n",
      "of language and desired states. This requires learning \"game AI\", encapsulated\n",
      "by our animation model, to navigate the scene using high-level constraints,\n",
      "play against an adversary, devise the strategy to win a point. The key to\n",
      "learning such game AI is the exploitation of a large and diverse text corpus,\n",
      "collected in this work, describing detailed actions in a game and used to train\n",
      "our animation model. To render the resulting state of the environment and its\n",
      "agents, we use a compositional NeRF representation used in our synthesis model.\n",
      "To foster future research, we present newly collected, annotated and calibrated\n",
      "large-scale Tennis and Minecraft datasets. Our method significantly outperforms\n",
      "existing neural video game simulators in terms of rendering quality. Besides,\n",
      "our LGEs unlock applications beyond capabilities of the current state of the\n",
      "art. Our framework, data, and models are available at\n",
      "https://learnable-game-engines.github.io/lge-website.\n",
      "http://arxiv.org/abs/2303.13471v1\n",
      "2023-03-23 17:43:11+00:00\n",
      "Egocentric Audio-Visual Object Localization\n",
      "Humans naturally perceive surrounding scenes by unifying sound and sight in a\n",
      "first-person view. Likewise, machines are advanced to approach human\n",
      "intelligence by learning with multisensory inputs from an egocentric\n",
      "perspective. In this paper, we explore the challenging egocentric audio-visual\n",
      "object localization task and observe that 1) egomotion commonly exists in\n",
      "first-person recordings, even within a short duration; 2) The out-of-view sound\n",
      "components can be created while wearers shift their attention. To address the\n",
      "first problem, we propose a geometry-aware temporal aggregation module to\n",
      "handle the egomotion explicitly. The effect of egomotion is mitigated by\n",
      "estimating the temporal geometry transformation and exploiting it to update\n",
      "visual representations. Moreover, we propose a cascaded feature enhancement\n",
      "module to tackle the second issue. It improves cross-modal localization\n",
      "robustness by disentangling visually-indicated audio representation. During\n",
      "training, we take advantage of the naturally available audio-visual temporal\n",
      "synchronization as the ``free'' self-supervision to avoid costly labeling. We\n",
      "also annotate and create the Epic Sounding Object dataset for evaluation\n",
      "purposes. Extensive experiments show that our method achieves state-of-the-art\n",
      "localization performance in egocentric videos and can be generalized to diverse\n",
      "audio-visual scenes.\n",
      "http://arxiv.org/abs/2303.13410v1\n",
      "2023-03-23 16:30:25+00:00\n",
      "Volume and Mass Conservation in Lagrangian Meshfree Methods\n",
      "Meshfree Lagrangian frameworks for free surface flow simulations do not\n",
      "conserve fluid volume. Meshfree particle methods like SPH are not mimetic, in\n",
      "the sense that discrete mass conservation does not imply discrete volume\n",
      "conservation. On the other hand, meshfree collocation methods typically do not\n",
      "use any notion of mass. As a result, they are neither mass conservative nor\n",
      "volume conservative at the discrete level. In this paper, we give an overview\n",
      "of various sources of conservation errors across different meshfree methods.\n",
      "The present work focuses on one specific issue: unreliable volume and mass\n",
      "definitions. We introduce the concept of representative masses and densities,\n",
      "which are essential for accurate post-processing especially in meshfree\n",
      "collocation methods. Using these, we introduce an artificial compression or\n",
      "expansion in the fluid to rectify errors in volume conservation. Numerical\n",
      "experiments show that the introduced frameworks significantly improve volume\n",
      "conservation behaviour, even for complex industrial test cases such as\n",
      "automotive water crossing.\n",
      "http://arxiv.org/abs/2303.13397v1\n",
      "2023-03-23 16:15:18+00:00\n",
      "DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video\n",
      "Human mesh recovery (HMR) provides rich human body information for various\n",
      "real-world applications such as gaming, human-computer interaction, and virtual\n",
      "reality. Compared to single image-based methods, video-based methods can\n",
      "utilize temporal information to further improve performance by incorporating\n",
      "human body motion priors. However, many-to-many approaches such as VIBE suffer\n",
      "from motion smoothness and temporal inconsistency. While many-to-one approaches\n",
      "such as TCMR and MPS-Net rely on the future frames, which is non-causal and\n",
      "time inefficient during inference. To address these challenges, a novel\n",
      "Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is\n",
      "presented. DDT is designed to decode specific motion patterns from the input\n",
      "sequence, enhancing motion smoothness and temporal consistency. As a\n",
      "many-to-many approach, the decoder of our DDT outputs the human mesh of all the\n",
      "frames, making DDT more viable for real-world applications where time\n",
      "efficiency is crucial and a causal model is desired. Extensive experiments are\n",
      "conducted on the widely used datasets (Human3.6M, MPI-INF-3DHP, and 3DPW),\n",
      "which demonstrated the effectiveness and efficiency of our DDT.\n",
      "http://arxiv.org/abs/2303.13385v1\n",
      "2023-03-23 15:55:19+00:00\n",
      "Planning for Manipulation among Movable Objects: Deciding Which Objects Go Where, in What Order, and How\n",
      "We are interested in pick-and-place style robot manipulation tasks in\n",
      "cluttered and confined 3D workspaces among movable objects that may be\n",
      "rearranged by the robot and may slide, tilt, lean or topple. A recently\n",
      "proposed algorithm, M4M, determines which objects need to be moved and where by\n",
      "solving a Multi-Agent Pathfinding MAPF abstraction of this problem. It then\n",
      "utilises a nonprehensile push planner to compute actions for how the robot\n",
      "might realise these rearrangements and a rigid body physics simulator to check\n",
      "whether the actions satisfy physics constraints encoded in the problem.\n",
      "However, M4M greedily commits to valid pushes found during planning, and does\n",
      "not reason about orderings over pushes if multiple objects need to be\n",
      "rearranged. Furthermore, M4M does not reason about other possible MAPF\n",
      "solutions that lead to different rearrangements and pushes. In this paper, we\n",
      "extend M4M and present Enhanced-M4M (E-M4M) -- a systematic graph search-based\n",
      "solver that searches over orderings of pushes for movable objects that need to\n",
      "be rearranged and different possible rearrangements of the scene. We introduce\n",
      "several algorithmic optimisations to circumvent the increased computational\n",
      "complexity, discuss the space of problems solvable by E-M4M and show that\n",
      "experimentally, both on the real robot and in simulation, it significantly\n",
      "outperforms the original M4M algorithm, as well as other state-of-the-art\n",
      "alternatives when dealing with complex scenes.\n",
      "http://arxiv.org/abs/2303.13357v1\n",
      "2023-03-23 15:36:12+00:00\n",
      "POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery\n",
      "Transformer architectures have achieved SOTA performance on the human mesh\n",
      "recovery (HMR) from monocular images. However, the performance gain has come at\n",
      "the cost of substantial memory and computational overhead. A lightweight and\n",
      "efficient model to reconstruct accurate human mesh is needed for real-world\n",
      "applications. In this paper, we propose a pure transformer architecture named\n",
      "POoling aTtention TransformER (POTTER) for the HMR task from single images.\n",
      "Observing that the conventional attention module is memory and computationally\n",
      "expensive, we propose an efficient pooling attention module, which\n",
      "significantly reduces the memory and computational cost without sacrificing\n",
      "performance. Furthermore, we design a new transformer architecture by\n",
      "integrating a High-Resolution (HR) stream for the HMR task. The high-resolution\n",
      "local and global features from the HR stream can be utilized for recovering\n",
      "more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only\n",
      "requiring 7% of total parameters and 14% of the Multiply-Accumulate Operations\n",
      "on the Human3.6M (PA-MPJPE metric) and 3DPW (all three metrics) datasets. The\n",
      "project webpage is https://zczcwh.github.io/potter_page.\n",
      "http://arxiv.org/abs/2303.13352v1\n",
      "2023-03-23 15:29:27+00:00\n",
      "Planning for Complex Non-prehensile Manipulation Among Movable Objects by Interleaving Multi-Agent Pathfinding and Physics-Based Simulation\n",
      "Real-world manipulation problems in heavy clutter require robots to reason\n",
      "about potential contacts with objects in the environment. We focus on\n",
      "pick-and-place style tasks to retrieve a target object from a shelf where some\n",
      "`movable' objects must be rearranged in order to solve the task. In particular,\n",
      "our motivation is to allow the robot to reason over and consider non-prehensile\n",
      "rearrangement actions that lead to complex robot-object and object-object\n",
      "interactions where multiple objects might be moved by the robot simultaneously,\n",
      "and objects might tilt, lean on each other, or topple. To support this, we\n",
      "query a physics-based simulator to forward simulate these interaction dynamics\n",
      "which makes action evaluation during planning computationally very expensive.\n",
      "To make the planner tractable, we establish a connection between the domain of\n",
      "Manipulation Among Movable Objects and Multi-Agent Pathfinding that lets us\n",
      "decompose the problem into two phases our M4M algorithm iterates over. First we\n",
      "solve a multi-agent planning problem that reasons about the configurations of\n",
      "movable objects but does not forward simulate a physics model. Next, an arm\n",
      "motion planning problem is solved that uses a physics-based simulator but does\n",
      "not search over possible configurations of movable objects. We run simulated\n",
      "and real-world experiments with the PR2 robot and compare against relevant\n",
      "baseline algorithms. Our results highlight that M4M generates complex 3D\n",
      "interactions, and solves at least twice as many problems as the baselines with\n",
      "competitive performance.\n",
      "http://arxiv.org/abs/2303.13336v1\n",
      "2023-03-23 15:17:15+00:00\n",
      "Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI\n",
      "Generative AI has demonstrated impressive performance in various fields,\n",
      "among which speech synthesis is an interesting direction. With the diffusion\n",
      "model as the most popular generative model, numerous works have attempted two\n",
      "active tasks: text to speech and speech enhancement. This work conducts a\n",
      "survey on audio diffusion model, which is complementary to existing surveys\n",
      "that either lack the recent progress of diffusion-based speech synthesis or\n",
      "highlight an overall picture of applying diffusion model in multiple fields.\n",
      "Specifically, this work first briefly introduces the background of audio and\n",
      "diffusion model. As for the text-to-speech task, we divide the methods into\n",
      "three categories based on the stage where diffusion model is adopted: acoustic\n",
      "model, vocoder and end-to-end framework. Moreover, we categorize various speech\n",
      "enhancement tasks by either certain signals are removed or added into the input\n",
      "speech. Comparisons of experimental results and discussions are also covered in\n",
      "this survey.\n",
      "http://arxiv.org/abs/2303.13326v1\n",
      "2023-03-23 15:05:16+00:00\n",
      "Decentralized Adversarial Training over Graphs\n",
      "The vulnerability of machine learning models to adversarial attacks has been\n",
      "attracting considerable attention in recent years. Most existing studies focus\n",
      "on the behavior of stand-alone single-agent learners. In comparison, this work\n",
      "studies adversarial training over graphs, where individual agents are subjected\n",
      "to perturbations of varied strength levels across space. It is expected that\n",
      "interactions by linked agents, and the heterogeneity of the attack models that\n",
      "are possible over the graph, can help enhance robustness in view of the\n",
      "coordination power of the group. Using a min-max formulation of diffusion\n",
      "learning, we develop a decentralized adversarial training framework for\n",
      "multi-agent systems. We analyze the convergence properties of the proposed\n",
      "scheme for both convex and non-convex environments, and illustrate the enhanced\n",
      "robustness to adversarial attacks.\n"
     ]
    }
   ],
   "source": [
    "for result in search.results():\n",
    "    print(result.entry_id)\n",
    "    print(result.published)\n",
    "    print(result.title)\n",
    "    print(result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de4024",
   "metadata": {},
   "source": [
    "___\n",
    "## 02 - SQLite Database Setup\n",
    "- https://www.digitalocean.com/community/tutorials/how-to-use-the-sqlite3-module-in-python-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "073395dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection = sqlite3.connect(\"../data/abstracts.db\")\n",
    "# cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5c1306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x2183b230b90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create new table in database\n",
    "# cursor.execute(\"CREATE TABLE IF NOT EXISTS abstracts_ai (id TEXT PRIMARY KEY, \\\n",
    "#                                                          title TEXT, \\\n",
    "#                                                          date_published TEXT, \\\n",
    "#                                                          abstract TEXT)\"\n",
    "#               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19e8dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x2183b230b90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Insert dummy row\n",
    "# cursor.execute(\"INSERT INTO abstracts_ai VALUES ('a1', \\\n",
    "#                                                  'test_title', \\\n",
    "#                                                  '2023-02-16 18:16:09+00:00', \\\n",
    "#                                                  'test abstract text')\"\n",
    "#               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c8badb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>test_title</td>\n",
       "      <td>2023-02-16 18:16:09+00:00</td>\n",
       "      <td>test abstract text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       title             date_published            abstract\n",
       "0  a1  test_title  2023-02-16 18:16:09+00:00  test abstract text"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Fetch all rows\n",
    "# query = \"SELECT * FROM abstracts_ai\"\n",
    "# df = pd.read_sql_query(\"SELECT * FROM abstracts_ai\", connection)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2bcd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x2183b230b90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Delete dummy row\n",
    "# cursor.execute(\n",
    "#     \"DELETE FROM abstracts_ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12354b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, date_published, abstract]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Check all rows deleted\n",
    "# query = \"SELECT * FROM abstracts_ai\"\n",
    "# df = pd.read_sql_query(\"SELECT * FROM abstracts_ai\", connection)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa300ff3",
   "metadata": {},
   "source": [
    "___\n",
    "## 03 - Retrieve and Store arXiv AI Article Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2667d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in search.results():\n",
    "#     entry_id = result.entry_id\n",
    "#     uid = entry_id.split('.')[-1]\n",
    "#     title = result.title\n",
    "#     date_published = result.published\n",
    "#     abstract = result.summary\n",
    "    \n",
    "#     query = 'INSERT OR REPLACE INTO abstracts_ai(id, title, date_published, abstract)' + \\\n",
    "#             ' VALUES(?, ?, ?, ?);'\n",
    "    \n",
    "#     fields = (uid, title, date_published, abstract)\n",
    "\n",
    "#     cursor.execute(query, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21542f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05512v1</td>\n",
       "      <td>PAC-NeRF: Physics Augmented Continuum Neural R...</td>\n",
       "      <td>2023-03-09 18:59:50+00:00</td>\n",
       "      <td>Existing approaches to system identification (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05510v1</td>\n",
       "      <td>Planning with Large Language Models for Code G...</td>\n",
       "      <td>2023-03-09 18:59:47+00:00</td>\n",
       "      <td>Existing large language model-based code gener...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  05512v1  PAC-NeRF: Physics Augmented Continuum Neural R...   \n",
       "1  05510v1  Planning with Large Language Models for Code G...   \n",
       "\n",
       "              date_published  \\\n",
       "0  2023-03-09 18:59:50+00:00   \n",
       "1  2023-03-09 18:59:47+00:00   \n",
       "\n",
       "                                            abstract  \n",
       "0  Existing approaches to system identification (...  \n",
       "1  Existing large language model-based code gener...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Fetch all rows\n",
    "# query = \"SELECT * FROM abstracts_ai\"\n",
    "# df = pd.read_sql_query(\"SELECT * FROM abstracts_ai\", connection)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ca1d43",
   "metadata": {},
   "source": [
    "## Alternative - Without SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "396c6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d77d973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in search.results():\n",
    "    entry_id = result.entry_id\n",
    "    uid = entry_id.split('.')[-1]\n",
    "    title = result.title\n",
    "    date_published = result.published\n",
    "    abstract = result.summary\n",
    "    \n",
    "    result_dict = {'uid': uid,\n",
    "                   'title': title,\n",
    "                   'date_published': date_published,\n",
    "                   'abstract': abstract\n",
    "                  }\n",
    "    \n",
    "    df_raw = df_raw.append(result_dict, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69fae32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13519v1</td>\n",
       "      <td>Learning and Verification of Task Structure in...</td>\n",
       "      <td>2023-03-23 17:59:54+00:00</td>\n",
       "      <td>Given the enormous number of instructional vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13518v1</td>\n",
       "      <td>Three ways to improve feature alignment for op...</td>\n",
       "      <td>2023-03-23 17:59:53+00:00</td>\n",
       "      <td>The core problem in zero-shot open vocabulary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13512v1</td>\n",
       "      <td>Towards Solving Fuzzy Tasks with Human Feedbac...</td>\n",
       "      <td>2023-03-23 17:59:17+00:00</td>\n",
       "      <td>To facilitate research in the direction of fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13511v1</td>\n",
       "      <td>Neural Preset for Color Style Transfer</td>\n",
       "      <td>2023-03-23 17:59:10+00:00</td>\n",
       "      <td>In this paper, we present a Neural Preset tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13508v1</td>\n",
       "      <td>DreamBooth3D: Subject-Driven Text-to-3D Genera...</td>\n",
       "      <td>2023-03-23 17:59:00+00:00</td>\n",
       "      <td>We present DreamBooth3D, an approach to person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13497v1</td>\n",
       "      <td>TriPlaneNet: An Encoder for EG3D Inversion</td>\n",
       "      <td>2023-03-23 17:56:20+00:00</td>\n",
       "      <td>Recent progress in NeRF-based GANs has introdu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13496v1</td>\n",
       "      <td>The effectiveness of MAE pre-pretraining for b...</td>\n",
       "      <td>2023-03-23 17:56:12+00:00</td>\n",
       "      <td>This paper revisits the standard pretrain-then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13494v1</td>\n",
       "      <td>Attention! Dynamic Epistemic Logic Models of (...</td>\n",
       "      <td>2023-03-23 17:55:32+00:00</td>\n",
       "      <td>Attention is the crucial cognitive ability tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13489v1</td>\n",
       "      <td>Boosting Reinforcement Learning and Planning w...</td>\n",
       "      <td>2023-03-23 17:53:44+00:00</td>\n",
       "      <td>Although reinforcement learning has seen treme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13483v1</td>\n",
       "      <td>NS3D: Neuro-Symbolic Grounding of 3D Objects a...</td>\n",
       "      <td>2023-03-23 17:50:40+00:00</td>\n",
       "      <td>Grounding object properties and relations in 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13482v1</td>\n",
       "      <td>TactoFind: A Tactile Only System for Object Re...</td>\n",
       "      <td>2023-03-23 17:50:09+00:00</td>\n",
       "      <td>We study the problem of object retrieval in sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13472v1</td>\n",
       "      <td>Plotting Behind the Scenes: Towards Learnable ...</td>\n",
       "      <td>2023-03-23 17:43:17+00:00</td>\n",
       "      <td>Game engines are powerful tools in computer gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13471v1</td>\n",
       "      <td>Egocentric Audio-Visual Object Localization</td>\n",
       "      <td>2023-03-23 17:43:11+00:00</td>\n",
       "      <td>Humans naturally perceive surrounding scenes b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13410v1</td>\n",
       "      <td>Volume and Mass Conservation in Lagrangian Mes...</td>\n",
       "      <td>2023-03-23 16:30:25+00:00</td>\n",
       "      <td>Meshfree Lagrangian frameworks for free surfac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13397v1</td>\n",
       "      <td>DDT: A Diffusion-Driven Transformer-based Fram...</td>\n",
       "      <td>2023-03-23 16:15:18+00:00</td>\n",
       "      <td>Human mesh recovery (HMR) provides rich human ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13385v1</td>\n",
       "      <td>Planning for Manipulation among Movable Object...</td>\n",
       "      <td>2023-03-23 15:55:19+00:00</td>\n",
       "      <td>We are interested in pick-and-place style robo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13357v1</td>\n",
       "      <td>POTTER: Pooling Attention Transformer for Effi...</td>\n",
       "      <td>2023-03-23 15:36:12+00:00</td>\n",
       "      <td>Transformer architectures have achieved SOTA p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13352v1</td>\n",
       "      <td>Planning for Complex Non-prehensile Manipulati...</td>\n",
       "      <td>2023-03-23 15:29:27+00:00</td>\n",
       "      <td>Real-world manipulation problems in heavy clut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13336v1</td>\n",
       "      <td>Audio Diffusion Model for Speech Synthesis: A ...</td>\n",
       "      <td>2023-03-23 15:17:15+00:00</td>\n",
       "      <td>Generative AI has demonstrated impressive perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13326v1</td>\n",
       "      <td>Decentralized Adversarial Training over Graphs</td>\n",
       "      <td>2023-03-23 15:05:16+00:00</td>\n",
       "      <td>The vulnerability of machine learning models t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                              title  \\\n",
       "0   13519v1  Learning and Verification of Task Structure in...   \n",
       "1   13518v1  Three ways to improve feature alignment for op...   \n",
       "2   13512v1  Towards Solving Fuzzy Tasks with Human Feedbac...   \n",
       "3   13511v1             Neural Preset for Color Style Transfer   \n",
       "4   13508v1  DreamBooth3D: Subject-Driven Text-to-3D Genera...   \n",
       "5   13497v1         TriPlaneNet: An Encoder for EG3D Inversion   \n",
       "6   13496v1  The effectiveness of MAE pre-pretraining for b...   \n",
       "7   13494v1  Attention! Dynamic Epistemic Logic Models of (...   \n",
       "8   13489v1  Boosting Reinforcement Learning and Planning w...   \n",
       "9   13483v1  NS3D: Neuro-Symbolic Grounding of 3D Objects a...   \n",
       "10  13482v1  TactoFind: A Tactile Only System for Object Re...   \n",
       "11  13472v1  Plotting Behind the Scenes: Towards Learnable ...   \n",
       "12  13471v1        Egocentric Audio-Visual Object Localization   \n",
       "13  13410v1  Volume and Mass Conservation in Lagrangian Mes...   \n",
       "14  13397v1  DDT: A Diffusion-Driven Transformer-based Fram...   \n",
       "15  13385v1  Planning for Manipulation among Movable Object...   \n",
       "16  13357v1  POTTER: Pooling Attention Transformer for Effi...   \n",
       "17  13352v1  Planning for Complex Non-prehensile Manipulati...   \n",
       "18  13336v1  Audio Diffusion Model for Speech Synthesis: A ...   \n",
       "19  13326v1     Decentralized Adversarial Training over Graphs   \n",
       "\n",
       "              date_published  \\\n",
       "0  2023-03-23 17:59:54+00:00   \n",
       "1  2023-03-23 17:59:53+00:00   \n",
       "2  2023-03-23 17:59:17+00:00   \n",
       "3  2023-03-23 17:59:10+00:00   \n",
       "4  2023-03-23 17:59:00+00:00   \n",
       "5  2023-03-23 17:56:20+00:00   \n",
       "6  2023-03-23 17:56:12+00:00   \n",
       "7  2023-03-23 17:55:32+00:00   \n",
       "8  2023-03-23 17:53:44+00:00   \n",
       "9  2023-03-23 17:50:40+00:00   \n",
       "10 2023-03-23 17:50:09+00:00   \n",
       "11 2023-03-23 17:43:17+00:00   \n",
       "12 2023-03-23 17:43:11+00:00   \n",
       "13 2023-03-23 16:30:25+00:00   \n",
       "14 2023-03-23 16:15:18+00:00   \n",
       "15 2023-03-23 15:55:19+00:00   \n",
       "16 2023-03-23 15:36:12+00:00   \n",
       "17 2023-03-23 15:29:27+00:00   \n",
       "18 2023-03-23 15:17:15+00:00   \n",
       "19 2023-03-23 15:05:16+00:00   \n",
       "\n",
       "                                             abstract  \n",
       "0   Given the enormous number of instructional vid...  \n",
       "1   The core problem in zero-shot open vocabulary ...  \n",
       "2   To facilitate research in the direction of fin...  \n",
       "3   In this paper, we present a Neural Preset tech...  \n",
       "4   We present DreamBooth3D, an approach to person...  \n",
       "5   Recent progress in NeRF-based GANs has introdu...  \n",
       "6   This paper revisits the standard pretrain-then...  \n",
       "7   Attention is the crucial cognitive ability tha...  \n",
       "8   Although reinforcement learning has seen treme...  \n",
       "9   Grounding object properties and relations in 3...  \n",
       "10  We study the problem of object retrieval in sc...  \n",
       "11  Game engines are powerful tools in computer gr...  \n",
       "12  Humans naturally perceive surrounding scenes b...  \n",
       "13  Meshfree Lagrangian frameworks for free surfac...  \n",
       "14  Human mesh recovery (HMR) provides rich human ...  \n",
       "15  We are interested in pick-and-place style robo...  \n",
       "16  Transformer architectures have achieved SOTA p...  \n",
       "17  Real-world manipulation problems in heavy clut...  \n",
       "18  Generative AI has demonstrated impressive perf...  \n",
       "19  The vulnerability of machine learning models t...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c3528",
   "metadata": {},
   "source": [
    "___\n",
    "## 04 - DataFrame Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf3808f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid                            object\n",
      "title                          object\n",
      "date_published    datetime64[ns, UTC]\n",
      "abstract                       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c527aa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_published'] = pd.to_datetime(df['date_published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43e469b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid                            object\n",
      "title                          object\n",
      "date_published    datetime64[ns, UTC]\n",
      "abstract                       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5ed8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty column to store keyword extraction output\n",
    "df['keywords_and_scores'] = ''\n",
    "\n",
    "# Create empty column to store top keywords\n",
    "df['keywords'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48c2c4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords_and_scores</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13519v1</td>\n",
       "      <td>Learning and Verification of Task Structure in...</td>\n",
       "      <td>2023-03-23 17:59:54+00:00</td>\n",
       "      <td>Given the enormous number of instructional vid...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13518v1</td>\n",
       "      <td>Three ways to improve feature alignment for op...</td>\n",
       "      <td>2023-03-23 17:59:53+00:00</td>\n",
       "      <td>The core problem in zero-shot open vocabulary ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13512v1</td>\n",
       "      <td>Towards Solving Fuzzy Tasks with Human Feedbac...</td>\n",
       "      <td>2023-03-23 17:59:17+00:00</td>\n",
       "      <td>To facilitate research in the direction of fin...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13511v1</td>\n",
       "      <td>Neural Preset for Color Style Transfer</td>\n",
       "      <td>2023-03-23 17:59:10+00:00</td>\n",
       "      <td>In this paper, we present a Neural Preset tech...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13508v1</td>\n",
       "      <td>DreamBooth3D: Subject-Driven Text-to-3D Genera...</td>\n",
       "      <td>2023-03-23 17:59:00+00:00</td>\n",
       "      <td>We present DreamBooth3D, an approach to person...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13497v1</td>\n",
       "      <td>TriPlaneNet: An Encoder for EG3D Inversion</td>\n",
       "      <td>2023-03-23 17:56:20+00:00</td>\n",
       "      <td>Recent progress in NeRF-based GANs has introdu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13496v1</td>\n",
       "      <td>The effectiveness of MAE pre-pretraining for b...</td>\n",
       "      <td>2023-03-23 17:56:12+00:00</td>\n",
       "      <td>This paper revisits the standard pretrain-then...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13494v1</td>\n",
       "      <td>Attention! Dynamic Epistemic Logic Models of (...</td>\n",
       "      <td>2023-03-23 17:55:32+00:00</td>\n",
       "      <td>Attention is the crucial cognitive ability tha...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13489v1</td>\n",
       "      <td>Boosting Reinforcement Learning and Planning w...</td>\n",
       "      <td>2023-03-23 17:53:44+00:00</td>\n",
       "      <td>Although reinforcement learning has seen treme...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13483v1</td>\n",
       "      <td>NS3D: Neuro-Symbolic Grounding of 3D Objects a...</td>\n",
       "      <td>2023-03-23 17:50:40+00:00</td>\n",
       "      <td>Grounding object properties and relations in 3...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13482v1</td>\n",
       "      <td>TactoFind: A Tactile Only System for Object Re...</td>\n",
       "      <td>2023-03-23 17:50:09+00:00</td>\n",
       "      <td>We study the problem of object retrieval in sc...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13472v1</td>\n",
       "      <td>Plotting Behind the Scenes: Towards Learnable ...</td>\n",
       "      <td>2023-03-23 17:43:17+00:00</td>\n",
       "      <td>Game engines are powerful tools in computer gr...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13471v1</td>\n",
       "      <td>Egocentric Audio-Visual Object Localization</td>\n",
       "      <td>2023-03-23 17:43:11+00:00</td>\n",
       "      <td>Humans naturally perceive surrounding scenes b...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13410v1</td>\n",
       "      <td>Volume and Mass Conservation in Lagrangian Mes...</td>\n",
       "      <td>2023-03-23 16:30:25+00:00</td>\n",
       "      <td>Meshfree Lagrangian frameworks for free surfac...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13397v1</td>\n",
       "      <td>DDT: A Diffusion-Driven Transformer-based Fram...</td>\n",
       "      <td>2023-03-23 16:15:18+00:00</td>\n",
       "      <td>Human mesh recovery (HMR) provides rich human ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13385v1</td>\n",
       "      <td>Planning for Manipulation among Movable Object...</td>\n",
       "      <td>2023-03-23 15:55:19+00:00</td>\n",
       "      <td>We are interested in pick-and-place style robo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13357v1</td>\n",
       "      <td>POTTER: Pooling Attention Transformer for Effi...</td>\n",
       "      <td>2023-03-23 15:36:12+00:00</td>\n",
       "      <td>Transformer architectures have achieved SOTA p...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13352v1</td>\n",
       "      <td>Planning for Complex Non-prehensile Manipulati...</td>\n",
       "      <td>2023-03-23 15:29:27+00:00</td>\n",
       "      <td>Real-world manipulation problems in heavy clut...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13336v1</td>\n",
       "      <td>Audio Diffusion Model for Speech Synthesis: A ...</td>\n",
       "      <td>2023-03-23 15:17:15+00:00</td>\n",
       "      <td>Generative AI has demonstrated impressive perf...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13326v1</td>\n",
       "      <td>Decentralized Adversarial Training over Graphs</td>\n",
       "      <td>2023-03-23 15:05:16+00:00</td>\n",
       "      <td>The vulnerability of machine learning models t...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                              title  \\\n",
       "0   13519v1  Learning and Verification of Task Structure in...   \n",
       "1   13518v1  Three ways to improve feature alignment for op...   \n",
       "2   13512v1  Towards Solving Fuzzy Tasks with Human Feedbac...   \n",
       "3   13511v1             Neural Preset for Color Style Transfer   \n",
       "4   13508v1  DreamBooth3D: Subject-Driven Text-to-3D Genera...   \n",
       "5   13497v1         TriPlaneNet: An Encoder for EG3D Inversion   \n",
       "6   13496v1  The effectiveness of MAE pre-pretraining for b...   \n",
       "7   13494v1  Attention! Dynamic Epistemic Logic Models of (...   \n",
       "8   13489v1  Boosting Reinforcement Learning and Planning w...   \n",
       "9   13483v1  NS3D: Neuro-Symbolic Grounding of 3D Objects a...   \n",
       "10  13482v1  TactoFind: A Tactile Only System for Object Re...   \n",
       "11  13472v1  Plotting Behind the Scenes: Towards Learnable ...   \n",
       "12  13471v1        Egocentric Audio-Visual Object Localization   \n",
       "13  13410v1  Volume and Mass Conservation in Lagrangian Mes...   \n",
       "14  13397v1  DDT: A Diffusion-Driven Transformer-based Fram...   \n",
       "15  13385v1  Planning for Manipulation among Movable Object...   \n",
       "16  13357v1  POTTER: Pooling Attention Transformer for Effi...   \n",
       "17  13352v1  Planning for Complex Non-prehensile Manipulati...   \n",
       "18  13336v1  Audio Diffusion Model for Speech Synthesis: A ...   \n",
       "19  13326v1     Decentralized Adversarial Training over Graphs   \n",
       "\n",
       "              date_published  \\\n",
       "0  2023-03-23 17:59:54+00:00   \n",
       "1  2023-03-23 17:59:53+00:00   \n",
       "2  2023-03-23 17:59:17+00:00   \n",
       "3  2023-03-23 17:59:10+00:00   \n",
       "4  2023-03-23 17:59:00+00:00   \n",
       "5  2023-03-23 17:56:20+00:00   \n",
       "6  2023-03-23 17:56:12+00:00   \n",
       "7  2023-03-23 17:55:32+00:00   \n",
       "8  2023-03-23 17:53:44+00:00   \n",
       "9  2023-03-23 17:50:40+00:00   \n",
       "10 2023-03-23 17:50:09+00:00   \n",
       "11 2023-03-23 17:43:17+00:00   \n",
       "12 2023-03-23 17:43:11+00:00   \n",
       "13 2023-03-23 16:30:25+00:00   \n",
       "14 2023-03-23 16:15:18+00:00   \n",
       "15 2023-03-23 15:55:19+00:00   \n",
       "16 2023-03-23 15:36:12+00:00   \n",
       "17 2023-03-23 15:29:27+00:00   \n",
       "18 2023-03-23 15:17:15+00:00   \n",
       "19 2023-03-23 15:05:16+00:00   \n",
       "\n",
       "                                             abstract keywords_and_scores  \\\n",
       "0   Given the enormous number of instructional vid...                       \n",
       "1   The core problem in zero-shot open vocabulary ...                       \n",
       "2   To facilitate research in the direction of fin...                       \n",
       "3   In this paper, we present a Neural Preset tech...                       \n",
       "4   We present DreamBooth3D, an approach to person...                       \n",
       "5   Recent progress in NeRF-based GANs has introdu...                       \n",
       "6   This paper revisits the standard pretrain-then...                       \n",
       "7   Attention is the crucial cognitive ability tha...                       \n",
       "8   Although reinforcement learning has seen treme...                       \n",
       "9   Grounding object properties and relations in 3...                       \n",
       "10  We study the problem of object retrieval in sc...                       \n",
       "11  Game engines are powerful tools in computer gr...                       \n",
       "12  Humans naturally perceive surrounding scenes b...                       \n",
       "13  Meshfree Lagrangian frameworks for free surfac...                       \n",
       "14  Human mesh recovery (HMR) provides rich human ...                       \n",
       "15  We are interested in pick-and-place style robo...                       \n",
       "16  Transformer architectures have achieved SOTA p...                       \n",
       "17  Real-world manipulation problems in heavy clut...                       \n",
       "18  Generative AI has demonstrated impressive perf...                       \n",
       "19  The vulnerability of machine learning models t...                       \n",
       "\n",
       "   keywords  \n",
       "0            \n",
       "1            \n",
       "2            \n",
       "3            \n",
       "4            \n",
       "5            \n",
       "6            \n",
       "7            \n",
       "8            \n",
       "9            \n",
       "10           \n",
       "11           \n",
       "12           \n",
       "13           \n",
       "14           \n",
       "15           \n",
       "16           \n",
       "17           \n",
       "18           \n",
       "19           "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce627d6",
   "metadata": {},
   "source": [
    "___\n",
    "## 05 - Keyword Extraction with KeyBERT\n",
    "- https://github.com/MaartenGr/KeyBERT\n",
    "- https://maartengr.github.io/KeyBERT/guides/embeddings.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43b85cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using 'all-MiniLM-L6-v2' given its speed and good quality\n",
    "# https://www.sbert.net/docs/pretrained_models.html#model-overview\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19d28b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "stop_words = 'english'\n",
    "ngram_lower_bound = 1\n",
    "ngram_upper_bound = 2\n",
    "use_mmr = True\n",
    "diversity = 0.1\n",
    "use_maxsum=False\n",
    "nr_candidates = 20\n",
    "top_n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7028f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    abstract_text = row['abstract']\n",
    "    kw_output = kw_model.extract_keywords(abstract_text, \n",
    "                                  keyphrase_ngram_range=(ngram_lower_bound, ngram_upper_bound), \n",
    "                                  stop_words=stop_words,\n",
    "                                  use_mmr=use_mmr, \n",
    "                                  use_maxsum=use_maxsum,\n",
    "                                  diversity=diversity,\n",
    "                                  top_n=top_n)\n",
    "    df.at[i, 'keywords_and_scores'] = kw_output\n",
    "    \n",
    "    # Obtain keyword from every keyword-score pair\n",
    "    top_kw = []\n",
    "    \n",
    "    for pair in kw_output:\n",
    "        top_kw.append(pair[0])\n",
    "        \n",
    "    df.at[i, 'keywords'] = top_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9a959d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>date_published</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords_and_scores</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13519v1</td>\n",
       "      <td>Learning and Verification of Task Structure in...</td>\n",
       "      <td>2023-03-23 17:59:54+00:00</td>\n",
       "      <td>Given the enormous number of instructional vid...</td>\n",
       "      <td>[(train videotaskformer, 0.533), (learns step,...</td>\n",
       "      <td>[train videotaskformer, learns step, trained v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13518v1</td>\n",
       "      <td>Three ways to improve feature alignment for op...</td>\n",
       "      <td>2023-03-23 17:59:53+00:00</td>\n",
       "      <td>The core problem in zero-shot open vocabulary ...</td>\n",
       "      <td>[(detection training, 0.4193), (vocabulary det...</td>\n",
       "      <td>[detection training, vocabulary detection, zer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13512v1</td>\n",
       "      <td>Towards Solving Fuzzy Tasks with Human Feedbac...</td>\n",
       "      <td>2023-03-23 17:59:17+00:00</td>\n",
       "      <td>To facilitate research in the direction of fin...</td>\n",
       "      <td>[(minecraft competition, 0.5917), (human feedb...</td>\n",
       "      <td>[minecraft competition, human feedback, feedba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13511v1</td>\n",
       "      <td>Neural Preset for Color Style Transfer</td>\n",
       "      <td>2023-03-23 17:59:10+00:00</td>\n",
       "      <td>In this paper, we present a Neural Preset tech...</td>\n",
       "      <td>[(color normalization, 0.5621), (adaptive colo...</td>\n",
       "      <td>[color normalization, adaptive color, color ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13508v1</td>\n",
       "      <td>DreamBooth3D: Subject-Driven Text-to-3D Genera...</td>\n",
       "      <td>2023-03-23 17:59:00+00:00</td>\n",
       "      <td>We present DreamBooth3D, an approach to person...</td>\n",
       "      <td>[(3d generative, 0.5317), (text 3d, 0.5204), (...</td>\n",
       "      <td>[3d generative, text 3d, 3d assets, dreambooth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13497v1</td>\n",
       "      <td>TriPlaneNet: An Encoder for EG3D Inversion</td>\n",
       "      <td>2023-03-23 17:56:20+00:00</td>\n",
       "      <td>Recent progress in NeRF-based GANs has introdu...</td>\n",
       "      <td>[(3d gans, 0.6795), (2d gan, 0.6178), (gan inv...</td>\n",
       "      <td>[3d gans, 2d gan, gan inversion, space gan, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13496v1</td>\n",
       "      <td>The effectiveness of MAE pre-pretraining for b...</td>\n",
       "      <td>2023-03-23 17:56:12+00:00</td>\n",
       "      <td>This paper revisits the standard pretrain-then...</td>\n",
       "      <td>[(models pretrained, 0.5777), (scale pretraini...</td>\n",
       "      <td>[models pretrained, scale pretraining, pretrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13494v1</td>\n",
       "      <td>Attention! Dynamic Epistemic Logic Models of (...</td>\n",
       "      <td>2023-03-23 17:55:32+00:00</td>\n",
       "      <td>Attention is the crucial cognitive ability tha...</td>\n",
       "      <td>[(propositional attention, 0.608), (attention ...</td>\n",
       "      <td>[propositional attention, attention axiomatiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13489v1</td>\n",
       "      <td>Boosting Reinforcement Learning and Planning w...</td>\n",
       "      <td>2023-03-23 17:53:44+00:00</td>\n",
       "      <td>Although reinforcement learning has seen treme...</td>\n",
       "      <td>[(demonstrations learning, 0.7309), (learning ...</td>\n",
       "      <td>[demonstrations learning, learning planning, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13483v1</td>\n",
       "      <td>NS3D: Neuro-Symbolic Grounding of 3D Objects a...</td>\n",
       "      <td>2023-03-23 17:50:40+00:00</td>\n",
       "      <td>Grounding object properties and relations in 3...</td>\n",
       "      <td>[(3d referring, 0.5461), (ns3d neuro, 0.5423),...</td>\n",
       "      <td>[3d referring, ns3d neuro, grounding ns3d, ns3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13482v1</td>\n",
       "      <td>TactoFind: A Tactile Only System for Object Re...</td>\n",
       "      <td>2023-03-23 17:50:09+00:00</td>\n",
       "      <td>We study the problem of object retrieval in sc...</td>\n",
       "      <td>[(sparse tactile, 0.5676), (grasping, 0.5123),...</td>\n",
       "      <td>[sparse tactile, grasping, tactile feedback, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13472v1</td>\n",
       "      <td>Plotting Behind the Scenes: Towards Learnable ...</td>\n",
       "      <td>2023-03-23 17:43:17+00:00</td>\n",
       "      <td>Game engines are powerful tools in computer gr...</td>\n",
       "      <td>[(game ai, 0.6155), (game engine, 0.5559), (le...</td>\n",
       "      <td>[game ai, game engine, learnable game, learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13471v1</td>\n",
       "      <td>Egocentric Audio-Visual Object Localization</td>\n",
       "      <td>2023-03-23 17:43:11+00:00</td>\n",
       "      <td>Humans naturally perceive surrounding scenes b...</td>\n",
       "      <td>[(egocentric audio, 0.6176), (egocentric video...</td>\n",
       "      <td>[egocentric audio, egocentric videos, inputs e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13410v1</td>\n",
       "      <td>Volume and Mass Conservation in Lagrangian Mes...</td>\n",
       "      <td>2023-03-23 16:30:25+00:00</td>\n",
       "      <td>Meshfree Lagrangian frameworks for free surfac...</td>\n",
       "      <td>[(meshfree methods, 0.6438), (volume meshfree,...</td>\n",
       "      <td>[meshfree methods, volume meshfree, meshfree c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13397v1</td>\n",
       "      <td>DDT: A Diffusion-Driven Transformer-based Fram...</td>\n",
       "      <td>2023-03-23 16:15:18+00:00</td>\n",
       "      <td>Human mesh recovery (HMR) provides rich human ...</td>\n",
       "      <td>[(ddt video, 0.4275), (human mesh, 0.407), (da...</td>\n",
       "      <td>[ddt video, human mesh, datasets human3, body ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13385v1</td>\n",
       "      <td>Planning for Manipulation among Movable Object...</td>\n",
       "      <td>2023-03-23 15:55:19+00:00</td>\n",
       "      <td>We are interested in pick-and-place style robo...</td>\n",
       "      <td>[(pushes planning, 0.5394), (robot manipulatio...</td>\n",
       "      <td>[pushes planning, robot manipulation, push pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13357v1</td>\n",
       "      <td>POTTER: Pooling Attention Transformer for Effi...</td>\n",
       "      <td>2023-03-23 15:36:12+00:00</td>\n",
       "      <td>Transformer architectures have achieved SOTA p...</td>\n",
       "      <td>[(attention transformer, 0.5762), (human mesh,...</td>\n",
       "      <td>[attention transformer, human mesh, attention ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13352v1</td>\n",
       "      <td>Planning for Complex Non-prehensile Manipulati...</td>\n",
       "      <td>2023-03-23 15:29:27+00:00</td>\n",
       "      <td>Real-world manipulation problems in heavy clut...</td>\n",
       "      <td>[(planning computationally, 0.5832), (motion p...</td>\n",
       "      <td>[planning computationally, motion planning, ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13336v1</td>\n",
       "      <td>Audio Diffusion Model for Speech Synthesis: A ...</td>\n",
       "      <td>2023-03-23 15:17:15+00:00</td>\n",
       "      <td>Generative AI has demonstrated impressive perf...</td>\n",
       "      <td>[(speech enhancement, 0.6165), (speech synthes...</td>\n",
       "      <td>[speech enhancement, speech synthesis, audio d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13326v1</td>\n",
       "      <td>Decentralized Adversarial Training over Graphs</td>\n",
       "      <td>2023-03-23 15:05:16+00:00</td>\n",
       "      <td>The vulnerability of machine learning models t...</td>\n",
       "      <td>[(decentralized adversarial, 0.6172), (adversa...</td>\n",
       "      <td>[decentralized adversarial, adversarial traini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                              title  \\\n",
       "0   13519v1  Learning and Verification of Task Structure in...   \n",
       "1   13518v1  Three ways to improve feature alignment for op...   \n",
       "2   13512v1  Towards Solving Fuzzy Tasks with Human Feedbac...   \n",
       "3   13511v1             Neural Preset for Color Style Transfer   \n",
       "4   13508v1  DreamBooth3D: Subject-Driven Text-to-3D Genera...   \n",
       "5   13497v1         TriPlaneNet: An Encoder for EG3D Inversion   \n",
       "6   13496v1  The effectiveness of MAE pre-pretraining for b...   \n",
       "7   13494v1  Attention! Dynamic Epistemic Logic Models of (...   \n",
       "8   13489v1  Boosting Reinforcement Learning and Planning w...   \n",
       "9   13483v1  NS3D: Neuro-Symbolic Grounding of 3D Objects a...   \n",
       "10  13482v1  TactoFind: A Tactile Only System for Object Re...   \n",
       "11  13472v1  Plotting Behind the Scenes: Towards Learnable ...   \n",
       "12  13471v1        Egocentric Audio-Visual Object Localization   \n",
       "13  13410v1  Volume and Mass Conservation in Lagrangian Mes...   \n",
       "14  13397v1  DDT: A Diffusion-Driven Transformer-based Fram...   \n",
       "15  13385v1  Planning for Manipulation among Movable Object...   \n",
       "16  13357v1  POTTER: Pooling Attention Transformer for Effi...   \n",
       "17  13352v1  Planning for Complex Non-prehensile Manipulati...   \n",
       "18  13336v1  Audio Diffusion Model for Speech Synthesis: A ...   \n",
       "19  13326v1     Decentralized Adversarial Training over Graphs   \n",
       "\n",
       "              date_published  \\\n",
       "0  2023-03-23 17:59:54+00:00   \n",
       "1  2023-03-23 17:59:53+00:00   \n",
       "2  2023-03-23 17:59:17+00:00   \n",
       "3  2023-03-23 17:59:10+00:00   \n",
       "4  2023-03-23 17:59:00+00:00   \n",
       "5  2023-03-23 17:56:20+00:00   \n",
       "6  2023-03-23 17:56:12+00:00   \n",
       "7  2023-03-23 17:55:32+00:00   \n",
       "8  2023-03-23 17:53:44+00:00   \n",
       "9  2023-03-23 17:50:40+00:00   \n",
       "10 2023-03-23 17:50:09+00:00   \n",
       "11 2023-03-23 17:43:17+00:00   \n",
       "12 2023-03-23 17:43:11+00:00   \n",
       "13 2023-03-23 16:30:25+00:00   \n",
       "14 2023-03-23 16:15:18+00:00   \n",
       "15 2023-03-23 15:55:19+00:00   \n",
       "16 2023-03-23 15:36:12+00:00   \n",
       "17 2023-03-23 15:29:27+00:00   \n",
       "18 2023-03-23 15:17:15+00:00   \n",
       "19 2023-03-23 15:05:16+00:00   \n",
       "\n",
       "                                             abstract  \\\n",
       "0   Given the enormous number of instructional vid...   \n",
       "1   The core problem in zero-shot open vocabulary ...   \n",
       "2   To facilitate research in the direction of fin...   \n",
       "3   In this paper, we present a Neural Preset tech...   \n",
       "4   We present DreamBooth3D, an approach to person...   \n",
       "5   Recent progress in NeRF-based GANs has introdu...   \n",
       "6   This paper revisits the standard pretrain-then...   \n",
       "7   Attention is the crucial cognitive ability tha...   \n",
       "8   Although reinforcement learning has seen treme...   \n",
       "9   Grounding object properties and relations in 3...   \n",
       "10  We study the problem of object retrieval in sc...   \n",
       "11  Game engines are powerful tools in computer gr...   \n",
       "12  Humans naturally perceive surrounding scenes b...   \n",
       "13  Meshfree Lagrangian frameworks for free surfac...   \n",
       "14  Human mesh recovery (HMR) provides rich human ...   \n",
       "15  We are interested in pick-and-place style robo...   \n",
       "16  Transformer architectures have achieved SOTA p...   \n",
       "17  Real-world manipulation problems in heavy clut...   \n",
       "18  Generative AI has demonstrated impressive perf...   \n",
       "19  The vulnerability of machine learning models t...   \n",
       "\n",
       "                                  keywords_and_scores  \\\n",
       "0   [(train videotaskformer, 0.533), (learns step,...   \n",
       "1   [(detection training, 0.4193), (vocabulary det...   \n",
       "2   [(minecraft competition, 0.5917), (human feedb...   \n",
       "3   [(color normalization, 0.5621), (adaptive colo...   \n",
       "4   [(3d generative, 0.5317), (text 3d, 0.5204), (...   \n",
       "5   [(3d gans, 0.6795), (2d gan, 0.6178), (gan inv...   \n",
       "6   [(models pretrained, 0.5777), (scale pretraini...   \n",
       "7   [(propositional attention, 0.608), (attention ...   \n",
       "8   [(demonstrations learning, 0.7309), (learning ...   \n",
       "9   [(3d referring, 0.5461), (ns3d neuro, 0.5423),...   \n",
       "10  [(sparse tactile, 0.5676), (grasping, 0.5123),...   \n",
       "11  [(game ai, 0.6155), (game engine, 0.5559), (le...   \n",
       "12  [(egocentric audio, 0.6176), (egocentric video...   \n",
       "13  [(meshfree methods, 0.6438), (volume meshfree,...   \n",
       "14  [(ddt video, 0.4275), (human mesh, 0.407), (da...   \n",
       "15  [(pushes planning, 0.5394), (robot manipulatio...   \n",
       "16  [(attention transformer, 0.5762), (human mesh,...   \n",
       "17  [(planning computationally, 0.5832), (motion p...   \n",
       "18  [(speech enhancement, 0.6165), (speech synthes...   \n",
       "19  [(decentralized adversarial, 0.6172), (adversa...   \n",
       "\n",
       "                                             keywords  \n",
       "0   [train videotaskformer, learns step, trained v...  \n",
       "1   [detection training, vocabulary detection, zer...  \n",
       "2   [minecraft competition, human feedback, feedba...  \n",
       "3   [color normalization, adaptive color, color ma...  \n",
       "4   [3d generative, text 3d, 3d assets, dreambooth...  \n",
       "5   [3d gans, 2d gan, gan inversion, space gan, ba...  \n",
       "6   [models pretrained, scale pretraining, pretrai...  \n",
       "7   [propositional attention, attention axiomatiza...  \n",
       "8   [demonstrations learning, learning planning, d...  \n",
       "9   [3d referring, ns3d neuro, grounding ns3d, ns3...  \n",
       "10  [sparse tactile, grasping, tactile feedback, r...  \n",
       "11  [game ai, game engine, learnable game, learnin...  \n",
       "12  [egocentric audio, egocentric videos, inputs e...  \n",
       "13  [meshfree methods, volume meshfree, meshfree c...  \n",
       "14  [ddt video, human mesh, datasets human3, body ...  \n",
       "15  [pushes planning, robot manipulation, push pla...  \n",
       "16  [attention transformer, human mesh, attention ...  \n",
       "17  [planning computationally, motion planning, ag...  \n",
       "18  [speech enhancement, speech synthesis, audio d...  \n",
       "19  [decentralized adversarial, adversarial traini...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6592de",
   "metadata": {},
   "source": [
    "### Get value counts of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86b793b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human mesh</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train videotaskformer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meshfree particle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>person recordings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>object localization</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meshfree methods</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>volume meshfree</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>meshfree collocation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meshfree lagrangian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>meshfree</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 keyword  count\n",
       "0             human mesh      2\n",
       "1  train videotaskformer      1\n",
       "2      meshfree particle      1\n",
       "3      person recordings      1\n",
       "4    object localization      1\n",
       "5       meshfree methods      1\n",
       "6        volume meshfree      1\n",
       "7   meshfree collocation      1\n",
       "8    meshfree lagrangian      1\n",
       "9               meshfree      1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_count = pd.DataFrame(pd.Series([x for item in df.keywords for x in item]).value_counts()).reset_index()\n",
    "keywords_count.columns = ['keyword', 'count']\n",
    "keywords_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3bf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686969c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keybert_taipy_venv",
   "language": "python",
   "name": "keybert_taipy_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
